---
title: "Examen Minería de Datos"
author: "Kenny Yahir Méndez Ramírez"
date: "19 de junio de 2016"
output: pdf_document
---

## 3. Clasificador ingenuo de Bayes

- ¿Cuál es su predicción para las siguientes tuplas?
	- ($outlook$ = rain, $Temperature$ = 73, $Humidity$ = 82, $wind$ = `TRUE`)
	- ($outlook$ = sunny, $Temperature$ = 72, $Humidity$ = 65, $wind$ = `TRUE`)
	

Se toman los datos de un archivo `csv`:
	
```{r message = FALSE, warning = FALSE}
library(e1071) # Se encuentra naiveBayes
library(knitr)

datos <- read.csv("Datos.csv")

testData <- data.frame(Outlook = factor(c("rain", "sunny"), levels = sort(unique((datos$Outlook)))), 
					   Temperature = c(73, 72),
					   Humidity = c(82, 65), 
					   wind = c(TRUE, TRUE))
```

Como se aprecia se agregan las 2 tuplas a predecir a un conjunto de datos de prueba. 
Lo siguiente es construir el modelo y se realiza el cálculo mediante el algoritmo ingenuo de bayes:

```{r echo = TRUE, message = FALSE, warning = FALSE}

modelo <- naiveBayes(Play ~ . , data = datos)

result <- predict(modelo, newdata = testData)
testData$Play <- result

kable(testData)
```

Para ambos casos la respuesta es $Play$ = `yes`.

- Se adapta muy bien para variables del tipo categóricas y su proceso es muy intuitivo, por ende su fácil comprensión para las personas. Pueden manejar un gran volumen de datos. Entre sus desventajas están que dependiendo del dominio de las variables a tratar éstas requieran un preprocesamiento que impacten a la naturaleza del problema original.

- El algoritmo de Naive Bayes también es muy fácil de entender, y llevar a cabo. Una potencial desventaja, y me atrevo a decir que aún más que el de construir árboles de decisión, es que nos basamos en el supuesto de que nuestras variables son independientes condicionalmente entre sí. 


```{r echo = FALSE, message = FALSE}
library(ggplot2)

temaScatter <- theme(panel.background = element_blank(), # Tema para la grafica
			  panel.grid.major = element_line(colour = "gray", linetype = "solid"),
			  #panel.grid.major.x = element_blank(), # Es necesario para desactivar las mallas verticales
			  axis.ticks = element_blank(),
			  axis.text = element_text(size = 9),
			  #axis.text.x = element_text(angle = 32),
			  axis.title = element_text(size = 10, face = "bold"),
			  axis.line.x = element_line(color = "black"),
			  legend.text = element_text(size = 9),
			  legend.title = element_text(size = 10),
			  legend.key = element_blank(),
			  title = element_text(face = "bold", size = 10)
)

discre <- function(x, y){
	if(is.factor(x))
		return (x == y)
	else  
		(x - y)^2
} # Devuelve la diferencia entre 2 elementos in R^1

my_dist <- function(X, Y){ # X y Y son vectores
	
	diffs <- mapply(discre , X, Y)
	
	sum(diffs)
	
} # Devuelve un escalar (distancia)

normaliza <- function(x){(x - min(x)) / (max(x) - min(x))} # normaliza un vector
```

## 4. K-nn

a) Utilizando las primeras 8 instancias como datos de entrenamiento, cuál es su predicción para la instancia 9 utilizando K-nn, en modo reagrupamiento general. Realice una propuesta para el valor de $K$

Leemos los datos

```{r message = FALSE}
datos <- read.csv("Datos2.csv")

datos
```

Se normalizan y se divide en un conjunto de entrenamiento y la tupla a predecir se establece como de prueba:

```{r}
datos$a2 <- normaliza(datos$a2)
datos$a3 <- normaliza(datos$a3)


trainData <- datos[1:8, 2:3]
testData <- datos[9, 2:3]

kable(trainData)
kable(testData)
```

Se puede observar que solo tomamos las variables `a2` y `a3` para los conjuntos de datos, esto lo hicimos porque al no contar con mayor información de las variables, no podemos considerar la variable `a1` (al ser categórica) con el mismo peso que las otras. De manera visual también se puede observar que con éstas 2 variables se aprecian los posibles grupos:


```{r ,message = FALSE, fig.align = 'center'}

plt1 <- ggplot(datos, aes(x = a2, y = a3, color = factor(clase))) + geom_point(size = 3)
plt1 + temaScatter
```

(La tupla a etiquetar (obs 9) es el punto oscuro mostrado en la gráfica de arriba).


Ahora realizamos los cálculos y decidimos como etiquetar dicha tupla tomando $K = 3$:

```{r message = FALSE}
arr_dist <- data.frame(val = -1, clase = datos$clase[1:8])

for(i in 1:nrow(trainData))
{
	arr_dist[i, "val"] <- my_dist(testData[1, ], trainData[i, ])
}

K <- 3

primerosK <- (arr_dist[order(arr_dist$val, decreasing = F), ])[1:K, ]

sum(primerosK$clase == 1) / nrow(primerosK)
```

Vemos que la proporción de observaciones que pertenecen a la clase 1 es de $0.33$ por lo que la clase a la que asignamos la tupla es a la **clase 0**.

b) Reagrupamiento por clase

```{r message = FALSE, echo = FALSE}
grupo1 <- datos[datos$clase == 1 & !is.na(datos$clase), 2:3]
grupo2 <- datos[datos$clase == 0 & !is.na(datos$clase), 2:3]

K <- 3

arr_dist_gp1 <- data.frame(val = 0, clase = rep(1, nrow(grupo1))) 

for(i in 1:nrow(grupo1)){
	arr_dist_gp1[i, "val"] <- my_dist(testData[1, ], grupo1[i, ]) 
}




arr_dist_gp2 <- data.frame(val = 0, clase = rep(0, nrow(grupo2)))


for(i in 1:nrow(grupo2)){
	arr_dist_gp2[i, "val"] <- my_dist(testData[1, ], grupo2[i, ]) 
}

(arr_dist_gp1[order(arr_dist_gp1$val), ])[K, ]

(arr_dist_gp2[order(arr_dist_gp2$val), ])[K, ]
```


Donde `val` representa la distancia al $K-ésimo$  vecino de cada clase hacia nuestra observación de interés. Al ser menor la distancia hacia el representante de la **clase 0** se dice que la tupla pertenece a esta clase.


## 6. Formación de conglomerados

```{r message = FALSE, echo = FALSE}
datos <- read.csv("Datos2.csv")
datos$a2 <- normaliza(datos$a2)
datos$a3 <- normaliza(datos$a3)
```

- a) Utilizando sólo las columnas `a2` y `a3` encuentre dos grupos utilizando el algoritmo de formación de grupos ascendente jerárquico.

Quitamos las etiquetas asignadas a nuestro conjunto de datos:

```{r message = FALSE}
datos

datosSinEtiquetas <- datos[, 2:3]

datosSinEtiquetas
```

Aplicamos el método de agrupamiento ascendente jerárquico y lo graficamos:

```{r message = FALSE, fig.align = 'center'}
agrupamiento <- hclust(dist(datosSinEtiquetas))
plot(agrupamiento)
```

Y vemos que las observaciones 2,5,8 quedan dentro de un grupo(etiqueta "2") y las demás son etiquetadas con "1".

```{r message = FALSE}
nDatos <- datos
nDatos$pred_jerar <- cutree(agrupamiento, 2)

kable(nDatos, row.names = TRUE)
```

- b) Ahora utilice el algoritmo de K-medias

Se crea la nueva columna de etiquetas acorde a K-medias con 2 centros:

```{r message = FALSE}
set.seed(7) # Fijamos una semilla
nDatos$pred_kmeans <- kmeans(datosSinEtiquetas, centers = 2)$cluster
nDatos
```

```{r message = FALSE, fig.align = 'center'}
library(ggplot2)

plt1 <- ggplot(data = nDatos, aes(x = a2, y = a3, color = factor(pred_kmeans))) + geom_point() + temaScatter
plt1
```



- c) Matriz de confusión

Se aplicó la matriz de confusión a ambas predicciones:

```{r message = FALSE, warning = FALSE}
res_kmeans <- nDatos$pred_kmeans - 1
res_jerar <- nDatos$pred_jerar - 1 # Se desplazan en una unidad: {1, 2} -> {0, 1}

library(caret) # Librería para usar la siguiente función

confusionMatrix(nDatos$clase, res_jerar)
confusionMatrix(nDatos$clase, res_kmeans)
```

En conclusión se obtiene mejor precisión usando K-medias para este conjunto de datos.